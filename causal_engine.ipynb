{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Grid Search for Target-Trial Emulation\n",
    "\n",
    "This cell executes a systematic search across different propensity score trimming and weighting configurations to identify stable causal effects of daily nutritional exposures on next-night sleep architecture. The framework follows contemporary epidemiology guidelines to separate causal effects from confounding in large observational datasets by emulating a \"target trial\".\n",
    "\n",
    "![Alt Text](https://media.giphy.com/media/v1.Y2lkPWVjZjA1ZTQ3dnAxMXQycXhkeW9iY2pna2lrMnk5bGlmbHFtMDd3OXFjcjRqdmlxYiZlcD12MV9naWZzX3NlYXJjaCZjdD1n/3o6MbeNr6v9XW7HNFS/giphy.gif)\n",
    "\n",
    "### The Search Space\n",
    "\n",
    "The grid explores two primary mechanisms used in the study to ensure the internal validity of the causal estimates: \n",
    "1. **Quantile Trimming (QUANTILE_GRID):**\n",
    "   **Purpose:** To address extreme propensity scores that can lead to high variance or bias in treatment effect estimates. \n",
    "   **Mechanism:** It excludes treated observations with very low propensity scores and control observations with very high propensity scores to ensure \"common support\". \n",
    "2. **Weight Clipping (CLIPPING_GRID)**\n",
    "   **Purpose:** To further stabilize the HÃ¡jek-type estimator by reducing the influence of extreme weights. \n",
    "   **Mechanism:** Weights are clipped at specified upper and lower percentiles to reduce the influence of extreme values.\n",
    "   \n",
    "Following the paper's robustness protocols, a result for a specific exposure is considered reliable only if it meets these conditions within the grid search: \n",
    "- **Covariate Balance:** Reaches an Absolute Standardized Mean Difference (ASMD) of $\\le 0.10$ for general confounders. Strict Structural Balance: Reaches an ASMD of $\\le 0.05$ for key structural confounders (Age, Sex, BMI) and baseline sleep characteristics.\n",
    "- **Stability:** Average Treatment Effect (ATE) estimates must be stable across reasonable trimming and weighting configurations.\n",
    "- **Negative Controls:** Shows no evidence of systematic treatment effects on food-insensitive outcomes like heart-rate signal quality or sleep position.\n",
    "\n",
    "### Execution Flow\n",
    "\n",
    "1. **Exposure Selection:** Iterates through 25 prespecified dietary exposures covering diet quality, macronutrients, micronutrients, and meal timing.\n",
    "2. **Propensity Scoring:** Fits a CatBoost gradient-boosted tree classifier to estimate the probability of exposure based on a comprehensive set of baseline and time-varying covariates.\n",
    "3. **Experimentation:** Evaluates combinations of trimming (q) and clipping (clip) to find a configuration that satisfies the strict balance and validity thresholds required for causal interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "plant_based_whole_foods_ratio_target_day\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexposure\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m base_cfg \u001b[38;5;241m=\u001b[39m replace(\n\u001b[1;32m     28\u001b[0m     BASE_CONFIG,\n\u001b[1;32m     29\u001b[0m     method\u001b[38;5;241m=\u001b[39mvals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     30\u001b[0m     limit\u001b[38;5;241m=\u001b[39mvals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcutoff\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     31\u001b[0m )\n\u001b[0;32m---> 33\u001b[0m df, kwargs, X, shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mget_propensity_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexposure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexposure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATAFRAME_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m passed_strict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m     41\u001b[0m     run_experiment(\n\u001b[1;32m     42\u001b[0m         config\u001b[38;5;241m=\u001b[39mreplace(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m clip \u001b[38;5;129;01min\u001b[39;00m CLIPPING_GRID\n\u001b[1;32m     56\u001b[0m )\n",
      "File \u001b[0;32m~/causal_framework/scripts/propensity.py:227\u001b[0m, in \u001b[0;36mget_propensity_scores\u001b[0;34m(exposure, config, variables, file)\u001b[0m\n\u001b[1;32m    217\u001b[0m df, kwargs \u001b[38;5;241m=\u001b[39m assign_treatment_values(\n\u001b[1;32m    218\u001b[0m     df,\n\u001b[1;32m    219\u001b[0m     exposure,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m: config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantile\u001b[39m\u001b[38;5;124m\"\u001b[39m]},\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[variables[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[0;32m--> 227\u001b[0m df, X, shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df, kwargs, X, shap_values\n",
      "File \u001b[0;32m~/causal_framework/scripts/propensity.py:202\u001b[0m, in \u001b[0;36mtrain_ps\u001b[0;34m(df, variables)\u001b[0m\n\u001b[1;32m    194\u001b[0m categorical, numerical, target \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    195\u001b[0m     variables[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    196\u001b[0m     variables[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumerical\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    197\u001b[0m     variables[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    198\u001b[0m )\n\u001b[1;32m    199\u001b[0m X, y, X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m create_pool(\n\u001b[1;32m    200\u001b[0m     df, categorical, numerical, target\n\u001b[1;32m    201\u001b[0m )\n\u001b[0;32m--> 202\u001b[0m cb \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m get_shap_summary(X[numerical \u001b[38;5;241m+\u001b[39m categorical], cb)\n\u001b[1;32m    204\u001b[0m X \u001b[38;5;241m=\u001b[39m calculate_propensity_scores(X, y, numerical, categorical, cb)\n",
      "File \u001b[0;32m~/causal_framework/scripts/propensity.py:150\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(X_train, y_train, X_test, y_test, categorical, verbose)\u001b[0m\n\u001b[1;32m    148\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(classes, weights))\n\u001b[1;32m    149\u001b[0m cb \u001b[38;5;241m=\u001b[39m CatBoostClassifier(class_weights\u001b[38;5;241m=\u001b[39mclass_weights, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 150\u001b[0m \u001b[43mcb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m cb\u001b[38;5;241m.\u001b[39mpredict(pool_test)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/catboost/core.py:5245\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m   5243\u001b[0m     CatBoostClassifier\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m-> 5245\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5246\u001b[0m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5247\u001b[0m \u001b[43m          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/catboost/core.py:2410\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2407\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2409\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[0;32m-> 2410\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_sets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[1;32m   2419\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/catboost/core.py:1790\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[0;32m-> 1790\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[0;32m_catboost.pyx:5023\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:5072\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from dataclasses import replace\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scripts.helpers import run_experiment\n",
    "from scripts.propensity import get_propensity_scores\n",
    "from variables.variables import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ================================================================\n",
    "# Hyperparameter grids\n",
    "# ================================================================\n",
    "\n",
    "QUANTILE_GRID = np.array([0.0, 0.001, 0.01, 0.0125, 0.015, 0.0175, 0.025, 0.05, 0.075])\n",
    "CLIPPING_PERCENTS = np.array([0.5, 1.0, 1.5, 2.5, 3.5, 5.0, 7.0, 7.5, 10.0, 12.5])\n",
    "CLIPPING_GRID = [(p, 100.0 - p) for p in CLIPPING_PERCENTS]\n",
    "\n",
    "# ================================================================\n",
    "# Loop\n",
    "# ================================================================\n",
    "\n",
    "for exposure, vals in EXPOSURES.items():\n",
    "    print(f\"\\n{exposure}\")\n",
    "\n",
    "    base_cfg = replace(\n",
    "        BASE_CONFIG,\n",
    "        method=vals[\"method\"],\n",
    "        limit=vals[\"cutoff\"],\n",
    "    )\n",
    "\n",
    "    df, kwargs, X, shap_values = get_propensity_scores(\n",
    "        exposure=exposure,\n",
    "        config=base_cfg.__dict__,\n",
    "        variables=variable_config,\n",
    "        file=DATAFRAME_PATH,\n",
    "    )\n",
    "\n",
    "    passed_strict = any(\n",
    "        run_experiment(\n",
    "            config=replace(\n",
    "                base_cfg,\n",
    "                q=float(q),\n",
    "                clip=clip,\n",
    "            ).__dict__,\n",
    "            variable_config=variable_config,\n",
    "            df=df,\n",
    "            kwargs=kwargs,\n",
    "            X=X,\n",
    "            shap_values=shap_values,\n",
    "        )\n",
    "        == \"PASS_STRICT\"\n",
    "        for q in QUANTILE_GRID\n",
    "        for clip in CLIPPING_GRID\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
